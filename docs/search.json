[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Target Trial Emulation and Causal Inference Tutorials",
    "section": "",
    "text": "R tutorials for implementing target trial emulation (TTE) and causal inference methods to estimate treatment effects using observational data. These tutorials introduce important concepts in TTE and provide step-by-step instructions for deploying the procedures with examples."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Welcome to Target Trial Emulation and Causal Inference Tutorials",
    "section": "",
    "text": "R tutorials for implementing target trial emulation (TTE) and causal inference methods to estimate treatment effects using observational data. These tutorials introduce important concepts in TTE and provide step-by-step instructions for deploying the procedures with examples."
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Welcome to Target Trial Emulation and Causal Inference Tutorials",
    "section": "Start here",
    "text": "Start here\n\nAll tutorials: See the tutorials listing\nOr jump to: Preparing Analysis Data by Emulating A Single Target Trial\nOr jump to: Preparing Analysis Data by Emulating A Sequence of Target Trials\nOr jump to: Introduction to Causal Inference and Intention to Treat Analysis\nOr jump to: Per-protocol Analysis\nOr jump to: Doubly Robust Estimation of Average Treatment Effects Using Pseudo Observations\nOr jump to: Augmented Inverse Probability Weighted Complete Case Estimator for Survival Outcomes\nOr jump to: Estimating the Variance of Inverse-Probability Weighted Risk Ratios Using Influence Functions"
  },
  {
    "objectID": "DRaipwcc.html",
    "href": "DRaipwcc.html",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#&gt;\",\n  message = FALSE,    \n  warning = FALSE \n)\n\nset.seed(123)\nlibrary(truncnorm)\nlibrary(survival)\nlibrary(adjustedCurves)\nlibrary(MASS)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dagitty)\nlibrary(parallel)\nlibrary(future.apply)\nlibrary(future)\n\nn_cores &lt;- detectCores() - 2"
  },
  {
    "objectID": "DRaipwcc.html#background",
    "href": "DRaipwcc.html#background",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "Background",
    "text": "Background\nWe continue our previous discussion on doubly robust (DR) estimators for time-to-event outcomes using pseudo observations but focus on the situation where the loss to follow-up depends on pre-baseline and post-baseline characteristics, referred to as informative censoring. For example, patients who are sicker at baseline or who develop treatment-related adverse events may be more likely to drop out early. In such situations, the censoring mechanism must be explicitly modeled as a function of baseline and time-dependent covariates."
  },
  {
    "objectID": "DRaipwcc.html#augmented-inverse-probability-weighted-complete-case-estimator",
    "href": "DRaipwcc.html#augmented-inverse-probability-weighted-complete-case-estimator",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "Augmented Inverse Probability Weighted Complete Case Estimator",
    "text": "Augmented Inverse Probability Weighted Complete Case Estimator\nWe use similar notations as before and consider \\(n\\) independent and identically distributed individuals indexed by \\(i\\). Let \\(D\\) denote a binary treatment (0 or 1), \\(T_d\\) denote the potential survival time under treatment \\(D=d\\), \\(C\\) the censoring time, \\(U_i=\\mathrm{min}(T_i, C_i)\\), and \\(\\boldsymbol{Z}\\) denotes pre-baseline covariates.\nLet \\(S_d(t)\\) be the survival function under treatment \\(d\\) at time \\(t\\). We define the average treatment effect (ATE) as the difference in survival probabilities between treatment groups at time \\(t\\), i.e., \\(S_1(t)-S_0(t)\\). Without loss of generality, we focus our illustration on estimating \\(S_1(t)\\), and \\(S_0(t)\\) can be estimated analogously.\nNote that we only get to observe \\(T_1\\) when \\(D=1\\) and \\(\\Delta = 1(C \\ge T_1)\\); otherwise \\(T_1\\) is missing or coarsened. Further, this is monotone coarsening (Tsiatis 2006) and we assume coarsening at random, i.e., \\((D,C) \\perp T_1|Z\\). Intuitively, coarsening in this case is induced by observed treatment and loss to follow-up, with treatment assignment depends only on pre-baseline characteristics and loss to follow-up relies on both pre- and post-baseline covariates, that is, informative censoring.\nTo handle bias induced by monotonic data coarsening, we apply the augmented inverse probability weighted Complete Case (AIPWCC) estimator described in Bai, Tsiatis, and O’Brien (2013). Specifically, the AIPWCC estimator\n\naccounts for coarsening due to treatment assignment using the propensity score model: \\(\\hat{e}(\\boldsymbol{Z})=P(D=1|Z)\\)\nhandles informative censoring using the treatment-specific censoring model: \\(\\hat{K}_1^c(r, Z) = P(C \\ge r|Z, D=1)\\)\n\nleverages the outcome regression model like in standard DR estimators: \\(\\hat{m}_1(\\boldsymbol{Z})=P(T_1&gt;r|Z)\\)\n\nThe optimal AIPWCC estimator is \\[ \\hat{S}_1(u) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\frac{D_i\\mathbb{1}(U_i\\ge u)}{\\hat{e}(Z_i)\\hat{K}_1^c(u, Z_i)}-\\left\\{\\frac{D_i-\\hat{e}(Z_i)}{\\hat{e}(Z_i)} \\hat{m}_1(u, Z_i)\\right\\}+\\int_0^u \\frac{D_i}{\\hat{e}(Z_i)}\\frac{\\mathrm{d}M_{1, i}^c(r, Z_i)}{\\hat{K}_1^c(r, Z_i)}\\frac{\\hat{m}_1(u, Z_i)}{\\hat{m}_1(r, Z_i)}\\right]\\] where \\(\\mathrm{d}M_{1, i}^c(r, Z_i) = \\frac{\\mathrm{d}N^c(r)-\\lambda_1^c(r, Z_i)Y(r)}{\\hat{K}_1^c(r, Z_i)}\\) is the martingale increment for the censoring distribution with \\(N^c(r)=\\mathbb{1}(U\\le r, \\Delta=0)\\), \\(Y(r)=\\mathbb{1}(U\\ge r)\\), and \\(\\lambda_1^c(r, Z_i)=\\frac{-\\mathrm{d} \\mathrm{log}\\hat{K}_1^c(r, Z_i)}{\\mathrm{d}r}\\).\nThe first term in the bracket in the equation above is the inverse probability weighted complete case term. The second term is the outcome-regression augmentation term in a standard DR estimator and has a mean of zero if the PS model is correct. The third term is the censoring augmentation term and also has a mean of zero if the censoring model is correct. The third term is essential to making the estimator robust to misspecification in the censoring model when the outcome model is correct.\nThe AIPWCC estimator is doubly robust:\n\nIf the coarsening models, including the propensity score model and censoring model, are correct\nIf the outcome model, that is, the conditional survival distribution is correct\n\nThe variance of the AIPWCC estimator for \\(\\hat{S}_1(u)\\) can be estimated using Equation (5) in Bai, Tsiatis, and O’Brien (2013). If all three models (PS model, censoring model, and outcome model) are correctly specified, the estimator is locally semiparametric efficient, meaning it attains the smallest possible variance. One caveat is that if the outcome model or coarsening models are misspecified, the AIPWCC estimator for ATE remains unbiased, but the standard sandwich variance estimator in Equation (5) is conservative. In such cases, a bootstrap estimator of the asymptotic variance is recommended."
  },
  {
    "objectID": "DRaipwcc.html#aipwcc-estimator-versus-pseudo-observation-based-dr-estimator",
    "href": "DRaipwcc.html#aipwcc-estimator-versus-pseudo-observation-based-dr-estimator",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "AIPWCC estimator versus pseudo-observation-based DR estimator",
    "text": "AIPWCC estimator versus pseudo-observation-based DR estimator\n\nThe pseudo-observation-based DR estimator proposed by Wang (2018) can only be applied when loss to follow-up is completely at random since the pseudo outcomes are estimated using the KM estimator where covariate adjustment is not allowed\nEven though the pseudo-observation-based DR estimator can be extended to handle covariate-dependent censoring (Binder, Gerds, and Andersen 2014), it models the censoring mechanism and outcome within the same model\nIn contrast, the AIPWCC estimator allows to model the censoring and outcome separately\nIn addition, the AIPWCC estimator includes an augmentation term for censoring to make it robust to censoring model misspecification and gain efficiency when the censoring model is correct"
  },
  {
    "objectID": "DRaipwcc.html#implementation-of-the-aipwcc-estimator",
    "href": "DRaipwcc.html#implementation-of-the-aipwcc-estimator",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "Implementation of the AIPWCC estimator",
    "text": "Implementation of the AIPWCC estimator\nWe now demonstrate how to implement the AIPWCC estimator under the informative censoring scenario.\n\nData Generation\nWe first simulate the data according to the variable relationships depicted in the directed acyclic graph (DAG) below.\n\n\nCode\ndag &lt;- dagitty(\"dag {\n  Z1 -&gt; Treatment\n  Z2 -&gt; Treatment\n  Z3 -&gt; Treatment\n  Z4 -&gt; Treatment\n  Z1 -&gt; Death\n  Z2 -&gt; Death\n  Z6 -&gt; Death\n  Z7 -&gt; Death\n  Treatment -&gt; Death\n}\")\n\ncoordinates(dag) &lt;- list(\n  x = c(Z3 = 0, Z4 = 0, Z1 = 1, Z2 = 1, Treatment = 1.5, Death = 3, Z6 = 4, Z7 = 4),\n  y = c(Z3 = 0.8, Z4 = 0.4, Z1 = 0.7, Z2 = 0.3, Treatment = 0.5, Death = 0.5, Z6 = 0.7, Z7 = 0.3)\n)\n\nplot(dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nalpha_0 &lt;- -0.5\nalpha_Z1 &lt;- 2\nalpha_Z2 &lt;- 0.5\nalpha_Z3 &lt;- 0.5\nalpha_Z4 &lt;- 0.5 \n\nbeta_0 &lt;- -2.55      \nbeta_D &lt;- -0.8 \nbeta_Z1 &lt;- 1.25 \nbeta_Z2 &lt;- 1.8\nbeta_Z6 &lt;- 1.8 \nbeta_Z7 &lt;- 1.3\n\ngenerate_data_inf &lt;- function(n = 500) {\n  Z1 &lt;- rtruncnorm(n, mean = 0, sd = 1, a = -0.5, b = 0.5)\n  Z2 &lt;- rnorm(n, 0, 1)\n  Z3 &lt;- rnorm(n, 0, 1)\n  Z4 &lt;- rnorm(n, 0, 1)\n  Z5 &lt;- rnorm(n, 0, 1)\n  Z6 &lt;- rnorm(n, 0, 1)\n  Z7 &lt;- rnorm(n, 0, 1)\n  Z8 &lt;- rnorm(n, 0, 1)\n  Z9 &lt;- rnorm(n, 0, 1)\n  \n  logit_p &lt;- alpha_0 + alpha_Z1 * Z1 + alpha_Z2 * Z2 + alpha_Z3 * Z3 + alpha_Z4 * Z4\n  p &lt;- plogis(logit_p) # exp(logit_p) / (1 + exp(logit_p))\n  D &lt;- rbinom(n, 1, p) # Treatment indicator\n  \n  hazard &lt;- exp(beta_0 + beta_D * D + beta_Z1 * Z1 + beta_Z2 * Z2 + beta_Z6 * Z6 + beta_Z7 * Z7)\n  T_event &lt;- rexp(n, rate = hazard)\n  \n  hazard_cen &lt;- exp(-2 + 0.05 * D + 0.05 * Z1 + 0.05 * Z2 + 0.05 * Z8 + 0.05 * Z9)\n  #hazard_cen &lt;- 0.0001\n  T_censor &lt;- rexp(n, rate = hazard_cen)\n  T_admin &lt;- rep(5, n)\n  \n  Time &lt;- pmin(T_event, T_censor, T_admin)\n  Event &lt;- as.numeric(T_event &lt;= T_censor & T_event &lt;= T_admin)\n  #Censored &lt;- as.numeric(T_event &gt; T_censor & T_event &lt;= T_admin) # Censoring indicator???\n  Censored &lt;- 1 - Event\n  \n  return(data.frame(Z1 = Z1, Z2 = Z2, Z3 = Z3, Z4 = Z4, Z5 = Z5, Z6 = Z6, Z7 = Z7, Z8 = Z8, Z9 = Z9, D = D, Time = Time, Event = Event, Censored = Censored))\n}\n\n\n\n\nCode\ndata &lt;- generate_data_inf()\n\nattr(data$Z1, \"label\") &lt;- \"Baseline value of Z1\"\nattr(data$Z2, \"label\") &lt;- \"Baseline value of Z2\"\nattr(data$Z3, \"label\") &lt;- \"Baseline value of Z3\"\nattr(data$Z4, \"label\") &lt;- \"Baseline value of Z4\"\nattr(data$Z5, \"label\") &lt;- \"Baseline value of Z5\"\nattr(data$Z6, \"label\") &lt;- \"Baseline value of Z6\"\nattr(data$Z7, \"label\") &lt;- \"Baseline value of Z7\"\nattr(data$Z8, \"label\") &lt;- \"Baseline value of Z8\"\nattr(data$Z9, \"label\") &lt;- \"Baseline value of Z9\"\nattr(data$D, \"label\") &lt;- \"Treatment indicator (1 = treated, 0 = control)\"\nattr(data$Time, \"label\") &lt;- \"Time index for longitudinal records\"\nattr(data$Event, \"label\") &lt;- \"Event indicator (1 = event occurred, 0 = otherwise)\"\n#attr(data$Censored, \"label\") &lt;- \"Censoring indicator (1 = censored, 0 = otherwise)\"???\n\nget_label &lt;- function(x) {\n  lbl &lt;- attr(x, \"label\", exact = TRUE)\n  if (is.null(lbl)) \"\" else as.character(lbl)\n}\n\ndict &lt;- data.frame(\n  Variable = names(data),\n  Meaning  = vapply(data, get_label, character(1)),\n  check.names = FALSE\n)\n\nknitr::kable(dict, caption = \"Data Dictionary\", row.names = FALSE) \n\n\n\nData Dictionary\n\n\nVariable\nMeaning\n\n\n\n\nZ1\nBaseline value of Z1\n\n\nZ2\nBaseline value of Z2\n\n\nZ3\nBaseline value of Z3\n\n\nZ4\nBaseline value of Z4\n\n\nZ5\nBaseline value of Z5\n\n\nZ6\nBaseline value of Z6\n\n\nZ7\nBaseline value of Z7\n\n\nZ8\nBaseline value of Z8\n\n\nZ9\nBaseline value of Z9\n\n\nD\nTreatment indicator (1 = treated, 0 = control)\n\n\nTime\nTime index for longitudinal records\n\n\nEvent\nEvent indicator (1 = event occurred, 0 = otherwise)\n\n\nCensored\n\n\n\n\n\n\n\n\nCode\nsummary(data$Time)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.000532 0.540231 2.077864 2.446897 5.000000 5.000000\n\ntable(data$Event)\n#&gt; \n#&gt;   0   1 \n#&gt; 320 180\n\ntable(data$D)\n#&gt; \n#&gt;   0   1 \n#&gt; 306 194\n\n\n\n\nCode\ncensored_subjects &lt;- data[data$Censored == 1, ]\n(early_dropout &lt;- sum(censored_subjects$Time &lt; 5))\n#&gt; [1] 182\n(admin_censored &lt;- sum(censored_subjects$Time &gt;= 5))\n#&gt; [1] 138\n\n\nAmong the 320 subjects with no events, 182 are early dropouts and 138 are administratively censored. Administrative censoring is random, so no additional modeling is needed. Early dropout could be covariate-dependent or occur completely at random, and we assume it is informative in this data generation.\nWith the simulation above, we can calculate the true survival probability.\n\n\nCode\nget_true_survival &lt;- function(data, t, D) {\n  hazard &lt;- exp(beta_0 + beta_D * D + beta_Z1 * data$Z1 + beta_Z2 * data$Z2 + beta_Z6 * data$Z6 + beta_Z7 * data$Z7)\n  survival_probs &lt;- exp(-hazard * t) # Baseline hazard is constant under an exponential survival model\n  return(mean(survival_probs)) \n}\n\n\nWe generate multiple datasets to compute the true ATEs at prespecified time points and corresponding 95% confidence intervals (CIs) using percentiles.\n\n\nCode\nn_datasets &lt;- 100\ndatasets &lt;- replicate(n_datasets, generate_data_inf(), simplify = FALSE)\n\ntimes &lt;- c(1, 2, 3)\ntrue_ate_samples &lt;- matrix(NA, nrow = n_datasets, ncol = length(times))\n\nfor (i in 1:n_datasets) {\n  # Compute potential survival probabilities by setting treatment to 1 and 0 for all individuals\n  true_surv_1 &lt;- sapply(times, function(t) get_true_survival(datasets[[i]], t, D = 1))\n  true_surv_0 &lt;- sapply(times, function(t) get_true_survival(datasets[[i]], t, D = 0))\n  \n  # Calculate ATE as difference in survival probabilities\n  true_ate_samples[i, ] &lt;- true_surv_1 - true_surv_0\n}\n\ntrue_ate_mean &lt;- colMeans(true_ate_samples)\ntrue_ate_ci_lower &lt;- apply(true_ate_samples, 2, quantile, probs = 0.025)\ntrue_ate_ci_upper &lt;- apply(true_ate_samples, 2, quantile, probs = 0.975)\n\nresults_true &lt;- list(ATE = true_ate_mean, ci_lower = true_ate_ci_lower, ci_upper = true_ate_ci_upper)\n\n\n\n\nUsing R package\nFirst, we implement the AIPWCC estimator using the adjustedsurv function in the adjustedCurves R package.\n\n\nCode\naipwcc_dr_ate_pkg &lt;- function(data, times, ps_correct = TRUE, or_correct = TRUE, \n                              cen_correct = TRUE, variance = \"asymptotic\") {\n  \n  data$D &lt;- factor(data$D, levels = c(0, 1))\n  \n  # Propensity score model\n  if (ps_correct) {\n    ps_model &lt;- glm(D ~ Z1 + Z2 + Z3 + Z4, data = data, family = \"binomial\")\n  } else {\n    ps_model &lt;- glm(D ~ Z5, data = data, family = \"binomial\")\n  }\n\n  # Censoring model \n  # Note: Event == 0 (Censored == 1) includes both loss to follow-up and administrative censoring\n  if (cen_correct) {\n    cen_model &lt;- coxph(Surv(Time, Event == 0) ~ D + Z1 + Z2 + Z8 + Z9, data = data, x = TRUE)\n  } else {\n    cen_model &lt;- coxph(Surv(Time, Event == 0) ~ D + Z5, data = data, x = TRUE)\n  }\n  \n  # Outcome regression (Cox proportional hazard model)\n  if (or_correct) {\n    or_model &lt;- coxph(Surv(Time, Event) ~ D + Z1 + Z2 + Z6 + Z7, data = data, x = TRUE)\n  } else {\n    or_model &lt;- coxph(Surv(Time, Event) ~ D + Z5, data = data, x = TRUE)\n  }\n  \n  # AIPWCC estimator for survival probabilities \n  if (variance == \"asymptotic\") { # Use asymptotic variance\n    adj_surv &lt;- adjustedsurv(\n      data = data,\n      variable = \"D\",\n      ev_time = \"Time\",\n      event = \"Event\",\n      method = \"aiptw\",\n      treatment_model = ps_model,\n      censoring_model = cen_model,\n      outcome_model = or_model,\n      times = times, \n      conf_int = TRUE,\n      conf_level = 0.95,\n      bootstrap = FALSE) \n  } else if (variance == \"bootstrap\"){ # Use bootstrap variance\n    adj_surv &lt;- adjustedsurv(\n      data = data,\n      variable = \"D\",\n      ev_time = \"Time\",\n      event = \"Event\",\n      method = \"aiptw\",\n      treatment_model = ps_model,\n      censoring_model = cen_model,\n      outcome_model = or_model,\n      times = times,\n      conf_int = TRUE,\n      conf_level = 0.95,\n      bootstrap = TRUE,\n      n_boot = 100) \n  }\n    \n  # Treatment effect is defined as the difference in survival probabilities.\n  ate_results &lt;- adjusted_curve_diff(\n    adj = adj_surv,\n    group_1 = \"1\",  # Treated group\n    group_2 = \"0\",  # Control group\n    conf_int = TRUE,\n    conf_level = 0.95)\n  \n  # Extract survival probabilities at times of interest\n  ate_at_times &lt;- ate_results[ate_results$time %in% times, ]\n  \n  return(list(ATE = ate_at_times$diff, ci_lower = ate_at_times$ci_lower, ci_upper = ate_at_times$ci_upper))\n}\n\n\nWe estimate the ATEs at prespecified time points and corresponding 95% confidence intervals (CIs).\n\n\nCode\nresults_asymp &lt;- aipwcc_dr_ate_pkg(data, times, variance = \"asymptotic\")\n# results_boot &lt;- aipwcc_dr_ate_pkg(data, times, variance = \"bootstrap\")\n# \n# results_package_tbl &lt;- data.frame(\n#   Time = paste0(\"T = \", times),\n#   True = sprintf(\"%.3f (%.3f, %.3f)\", results_true$ATE, results_true$ci_lower, results_true$ci_upper),\n#   Asymptotic = sprintf(\"%.3f (%.3f, %.3f)\", results_asymp$ATE, results_asymp$ci_lower, results_asymp$ci_upper),\n#   Bootstrap = sprintf(\"%.3f (%.3f, %.3f)\", results_boot$ATE, results_boot$ci_lower, results_boot$ci_upper)\n# )\n# \n# kable(results_package_tbl,\n#       caption = \"ATE (95% CI)\",\n#       col.names = c(\"Time\", \"True\", \"Asymptotic\", \"Bootstrap\"),\n#       align = c('c', 'c', 'c', 'c')) %&gt;%\n#   kable_styling(full_width = FALSE, font_size = 12)\n\n\n\n\nCrude estimation\nFit a crude Cox model with no covariate or propensity score adjustment to show the amount of confounding bias in our data\n\n\nCode\ncrude_cox_estimator &lt;- function(data, times) {\n  crude_cox &lt;- coxph(Surv(Time, Event) ~ D, data = data)\n  treatment_effect &lt;- coef(crude_cox)[\"D\"]\n  \n  base_surv &lt;- survfit(crude_cox)\n  s0 &lt;- summary(base_surv, times = times, extend = TRUE)$surv\n  \n  # Survival for treated group: S_baseline^exp(beta_D)\n  surv_1 &lt;- s0^exp(treatment_effect)\n  \n  # Survival for control group: S_baseline^exp(0) \n  surv_0 &lt;- s0\n\n  ATE &lt;- surv_1 - surv_0\n  return(list(ATE = ATE))\n}\n\nresults_crude &lt;- crude_cox_estimator(data, times)\n\n\n\n\nEvaluate the performance of AIPWCC estimator\nWe use the datasets generated previously to calculate RMSE, bias, and variance.\n\n\nCode\nplan(multisession, workers = n_cores)\n\ncalculate_metrics &lt;- function(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\", \n                              ps_correct = TRUE, or_correct = TRUE, cen_correct = TRUE) {\n  \n  n_datasets &lt;- length(datasets)\n  \n  ate &lt;- future_lapply(1:n_datasets, function(i) {\n    library(survival)\n    library(adjustedCurves)\n    \n    est_results &lt;- switch(\n      method,\n      \"package\" = aipwcc_dr_ate_pkg(datasets[[i]], times, variance = variance, \n                                    ps_correct = ps_correct, or_correct = or_correct, cen_correct = cen_correct),\n      \"crude\" = crude_cox_estimator(datasets[[i]], times),\n      stop(\"Unknown: \", method))\n    \n    est_results$ATE\n  }, future.seed = TRUE)\n  \n  ate_all &lt;- do.call(rbind, ate)\n  \n  bias &lt;- colMeans(ate_all - true_ate_samples)\n  variance &lt;- apply(ate_all, 2, var)\n  rmse &lt;- sqrt(colMeans((ate_all - true_ate_samples)^2))\n  \n  return(list(bias = bias, variance = variance, rmse = rmse))\n}\n\n\n\n\nModel misspecification scenarios\nTo illustrate the robustness of the AIPWCC estimator to model misspecification, we compare the following scenarios:\n\nScenario 1: The outcome model, the propensity score model, and the censoring model are all correctly specified\nScenario 2: The outcome model and propensity score model are correctly specified, while the censoring model is misspecified\nScenario 3: The outcome model and censoring model are correctly specified, while the propensity score model is misspecified\nScenario 4: The outcome model is correctly specified, while the propensity score model and censoring model are both misspecified\nScenario 5: The propensity score model and censoring model are correctly specified, but the outcome model is misspecified\nScenario 6: The propensity score model is correctly specified, but the outcome model and censoring model are both misspecified\nScenario 7: The censoring model is correctly specified, but the outcome model and propensity score model are both misspecified\nScenario 8: All three models are misspecified\n\n\n\nCode\n# Scenario 1: All models correct\nmetrics_1_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = TRUE, cen_correct = TRUE, or_correct = TRUE)\n# metrics_1_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = TRUE, cen_correct = TRUE, or_correct = TRUE)\n\n# Scenario 2: PS correct, Cen incorrect, OR correct\nmetrics_2_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = TRUE, cen_correct = FALSE, or_correct = TRUE)\n# metrics_2_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = TRUE, cen_correct = FALSE, or_correct = TRUE)\n\n# Scenario 3: PS incorrect, Cen correct, OR correct\nmetrics_3_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = FALSE, cen_correct = TRUE, or_correct = TRUE)\n# metrics_3_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = FALSE, cen_correct = TRUE, or_correct = TRUE)\n\n# Scenario 4: PS incorrect, Cen incorrect, OR correct\nmetrics_4_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = FALSE, cen_correct = FALSE, or_correct = TRUE)\n# metrics_4_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = FALSE, cen_correct = FALSE, or_correct = TRUE)\n\n# Scenario 5: PS correct, Cen correct, OR incorrect\nmetrics_5_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = TRUE, cen_correct = TRUE, or_correct = FALSE)\n# metrics_5_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = TRUE, cen_correct = TRUE, or_correct = FALSE)\n\n# Scenario 6: PS correct, Cen incorrect, OR incorrect\nmetrics_6_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = TRUE, cen_correct = FALSE, or_correct = FALSE)\n# metrics_6_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = TRUE, cen_correct = FALSE, or_correct = FALSE)\n\n# Scenario 7: PS incorrect, Cen correct, OR incorrect\nmetrics_7_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = FALSE, cen_correct = TRUE, or_correct = FALSE)\n# metrics_7_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = FALSE, cen_correct = TRUE, or_correct = FALSE)\n\n# Scenario 8: All models incorrect\nmetrics_8_asymp &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"asymptotic\",\n                                     ps_correct = FALSE, cen_correct = FALSE, or_correct = FALSE)\n# metrics_8_boot &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", variance = \"bootstrap\",\n#                                     ps_correct = FALSE, cen_correct = FALSE, or_correct = FALSE)\n\n# Scenario 0: Crude Cox\nmetrics_crude &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"crude\")\n\n\n\n\nCode\nscenarios &lt;- c(\"1: All Correct\",\n  \"2: PS Correct, Cen Incorrect, OR Correct\",\n  \"3: PS Incorrect, Cen Correct, OR Correct\",\n  \"4: PS Incorrect, Cen Incorrect, OR Correct\",\n  \"5: PS Correct, Cen Correct, OR Incorrect\",\n  \"6: PS Correct, Cen Incorrect, OR Incorrect\",\n  \"7: PS Incorrect, Cen Correct, OR Incorrect\",\n  \"8: All Incorrect\")\n\n# Combine values across all time points into a single string\nformat_values &lt;- function(vec) {\n  paste(sprintf(\"%.4f\", vec), collapse = \", \")\n}\n\n# RMSE table\nrmse_asymp &lt;- c(format_values(metrics_1_asymp$rmse), format_values(metrics_2_asymp$rmse), \n                format_values(metrics_3_asymp$rmse), format_values(metrics_4_asymp$rmse),\n                format_values(metrics_5_asymp$rmse), format_values(metrics_6_asymp$rmse), \n                format_values(metrics_7_asymp$rmse), format_values(metrics_8_asymp$rmse))\nrmse_asymp\n#&gt; [1] \"0.0290, 0.0279, 0.0295\" \"0.0290, 0.0279, 0.0296\" \"0.0278, 0.0282, 0.0294\"\n#&gt; [4] \"0.0277, 0.0281, 0.0294\" \"0.0401, 0.0411, 0.0428\" \"0.0400, 0.0409, 0.0427\"\n#&gt; [7] \"0.0985, 0.1058, 0.1113\" \"0.0984, 0.1058, 0.1112\"\n# rmse_boot &lt;- c(format_values(metrics_1_boot$rmse), format_values(metrics_2_boot$rmse),\n#                format_values(metrics_3_boot$rmse), format_values(metrics_4_boot$rmse),\n#                format_values(metrics_5_boot$rmse), format_values(metrics_6_boot$rmse),\n#                format_values(metrics_7_boot$rmse), format_values(metrics_8_boot$rmse))\n# \n# rmse_table &lt;- data.frame(Scenario = scenarios, Asymptotic = rmse_asymp, Bootstrap = rmse_boot)\n# \n# kable(rmse_table,\n#       col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n#       caption = \"Performance of the estimator (RMSE)\",\n#       align = c('c', 'c', 'c')) %&gt;%\n#   kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n#   add_header_above(c(\" \" = 1, \"Asymptotic\" = 1, \"Bootstrap\" = 1)) %&gt;%\n#   footnote(general = paste0(\"Crude Cox: \", format_values(metrics_crude$rmse)), general_title = \"\")\n\n\n\n\nCode\n# Bias table\nbias_asymp &lt;- c(format_values(metrics_1_asymp$bias), format_values(metrics_2_asymp$bias), \n                format_values(metrics_3_asymp$bias), format_values(metrics_4_asymp$bias),\n                format_values(metrics_5_asymp$bias), format_values(metrics_6_asymp$bias), \n                format_values(metrics_7_asymp$bias), format_values(metrics_8_asymp$bias))\nbias_asymp\n#&gt; [1] \"-0.0037, 0.0017, -0.0002\"  \"-0.0037, 0.0017, -0.0001\" \n#&gt; [3] \"-0.0049, 0.0005, 0.0008\"   \"-0.0048, 0.0004, 0.0007\"  \n#&gt; [5] \"-0.0051, 0.0006, -0.0011\"  \"-0.0050, 0.0006, -0.0010\" \n#&gt; [7] \"-0.0894, -0.0966, -0.1017\" \"-0.0893, -0.0966, -0.1017\"\n# bias_boot &lt;- c(format_values(metrics_1_boot$bias), format_values(metrics_2_boot$bias),\n#                format_values(metrics_3_boot$bias), format_values(metrics_4_boot$bias),\n#                format_values(metrics_5_boot$bias), format_values(metrics_6_boot$bias),\n#                format_values(metrics_7_boot$bias), format_values(metrics_8_boot$bias))\n# \n# bias_table &lt;- data.frame(Scenario = scenarios, Asymptotic = bias_asymp, Bootstrap = bias_boot)\n# \n# kable(bias_table,\n#       col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n#       caption = \"Performance of the estimator (Bias)\",\n#       align = c('c', 'c', 'c')) %&gt;%\n#   kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n#   add_header_above(c(\" \" = 1, \"Asymptotic\" = 1, \"Bootstrap\" = 1)) %&gt;%\n#   footnote(general = paste0(\"Crude Cox: \", format_values(metrics_crude$bias)), general_title = \"\")\n\n\n\n\nCode\n# Variance table\nvar_asymp &lt;- c(format_values(metrics_1_asymp$variance), format_values(metrics_2_asymp$variance), \n               format_values(metrics_3_asymp$variance), format_values(metrics_4_asymp$variance),\n               format_values(metrics_5_asymp$variance), format_values(metrics_6_asymp$variance), \n               format_values(metrics_7_asymp$variance), format_values(metrics_8_asymp$variance))\nvar_asymp\n#&gt; [1] \"0.0008, 0.0008, 0.0009\" \"0.0008, 0.0008, 0.0009\" \"0.0007, 0.0008, 0.0009\"\n#&gt; [4] \"0.0007, 0.0008, 0.0009\" \"0.0016, 0.0017, 0.0018\" \"0.0016, 0.0017, 0.0018\"\n#&gt; [7] \"0.0017, 0.0019, 0.0021\" \"0.0017, 0.0019, 0.0020\"\n# var_boot &lt;- c(format_values(metrics_1_boot$variance), format_values(metrics_2_boot$variance),\n#               format_values(metrics_3_boot$variance), format_values(metrics_4_boot$variance),\n#               format_values(metrics_5_boot$variance), format_values(metrics_6_boot$variance),\n#               format_values(metrics_7_boot$variance), format_values(metrics_8_boot$variance))\n# \n# var_table &lt;- data.frame(Scenario = scenarios, Asymptotic = var_asymp, Bootstrap = var_boot)\n# \n# kable(var_table,\n#       col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n#       caption = \"Performance of the estimator (Variance)\",\n#       align = c('c', 'c', 'c')) %&gt;%\n#   kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n#   add_header_above(c(\" \" = 1, \"Asymptotic\" = 1, \"Bootstrap\" = 1)) %&gt;%\n#   footnote(general = paste0(\"Crude Cox: \", format_values(metrics_crude$variance)), general_title = \"\")"
  },
  {
    "objectID": "DRaipwcc.html#funding",
    "href": "DRaipwcc.html#funding",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "Funding",
    "text": "Funding\nThis work was supported by Utah Clinical & Translational Science Institute (CTSI) Translational Innovation Pilot (TIP) Program Award (NCATS UM1TR004409)."
  },
  {
    "objectID": "DRaipwcc.html#footnotes",
    "href": "DRaipwcc.html#footnotes",
    "title": "Augmented Inverse Probability Weighted Complete Case Estimator for Survival Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎"
  },
  {
    "objectID": "DRpseudo.html",
    "href": "DRpseudo.html",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#&gt;\",\n  message = FALSE,    \n  warning = FALSE     \n)\n\nset.seed(123456)\nlibrary(truncnorm)\nlibrary(survival)\nlibrary(pseudo)\nlibrary(adjustedCurves)\nlibrary(MASS)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dagitty)\nlibrary(parallel)\nlibrary(future.apply)\nlibrary(future)\n\nn_cores &lt;- detectCores() - 2"
  },
  {
    "objectID": "DRpseudo.html#introduction",
    "href": "DRpseudo.html#introduction",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Introduction",
    "text": "Introduction\nAs noted in Funk et al. (2011), estimating causal effects of a treatment in observational studies is challenging due to confounding bias. Common approaches to controlling for confounding include outcome regression and propensity score methods.\n\nOutcome regression methods model the conditional expectation of the outcome given treatment and baseline covariates. Marginal outcome means for each treatment strategy are then obtained by setting treatment to “treated” or “control” for all subjects and averaging over the covariate distribution. The difference in these marginal means is a commonly used estimand for the average treatment effect (ATE).\nPropensity score methods first estimate the propensity score (PS), i.e., the probability of receiving treatment given covariates, and then apply inverse probability weighting to construct a pseudo-population in which treatment is independent of measured confounders.\n\nBoth approaches rely on correctly specified outcome or propensity score models, and misspecification of either can lead to biased estimates."
  },
  {
    "objectID": "DRpseudo.html#doubly-robust-estimation",
    "href": "DRpseudo.html#doubly-robust-estimation",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nDoubly robust (DR) estimation leverages both the outcome regression model and the propensity score model. Consider \\(n\\) independent and identically distributed individuals indexed by \\(i\\). Let \\(D\\) denote a binary treatment (0 or 1), \\(Y\\) the outcome of interest without censoring, and \\(\\boldsymbol{Z} = (Z_1, Z_2, \\ldots, Z_k)\\) a set of baseline covariates. Let \\(\\hat{e}(\\boldsymbol{Z})\\) be the estimated propensity score and \\(\\hat{m}_d(\\boldsymbol{Z})\\) be the predicted outcome given \\(D\\) and \\(\\boldsymbol{Z}\\). The DR estimator for the average treatment effect (ATE) is\n\\[\n\\hat{\\tau}_{DR} = \\frac{1}{n} \\sum_{i=1}^n\n\\left[\n  \\frac{D_i Y_i}{\\hat{e}(\\boldsymbol{Z}_i)}\n  - \\frac{(D_i - \\hat{e}(\\boldsymbol{Z}_i))}{\\hat{e}(\\boldsymbol{Z}_i)} \\hat{m}_1(\\boldsymbol{Z}_i)\n\\right]\n-\n\\left[\n  \\frac{(1 - D_i) Y_i}{1 - \\hat{e}(\\boldsymbol{Z}_i)}\n  + \\frac{(D_i - \\hat{e}(\\boldsymbol{Z}_i))}{1 - \\hat{e}(\\boldsymbol{Z}_i)} \\hat{m}_0(\\boldsymbol{Z}_i)\n\\right].\n\\]\nThe DR estimator is also known as the augmented inverse probability weighting (AIPW) estimator. In this form, the second term in each bracket serves as the augmentation term that converges to zero when either the PS model or outcome models are correctly specified. The doubly robustness property ensures that the estimator remains unbiased as long as either the PS model or the outcome models are correctly specified. Intuitively, The DR estimator adds another layer of protection against model misspecification, which is crucial in observational studies. For detailed mathematical derivations, please refer to Funk et al. (2011).\nIf both PS and outcome models are correctly specified, the estimator is semiparametric efficient (Tsiatis 2006), meaning it attains the smallest possible variance among all regular, asymptotically unbiased estimators for the ATE. One caveat is that if either model is misspecified, the DR estimator for ATE remains unbiased, but the resulting estimator for variance is biased. In such cases, a bootstrap estimator of the asymptotic variance is recommended (Bai, Tsiatis, and O’Brien 2013).\nThe DR estimator requires standard identifiability assumptions in causal inference, including the stable unit treatment value assumption (SUTVA), positivity, consistency, and exchangeability.\nFor doubly robust estimators for continuous and binary outcomes, please refer to the AIPW R package from Zhong et al. (2021)."
  },
  {
    "objectID": "DRpseudo.html#extension-to-survival-analysis",
    "href": "DRpseudo.html#extension-to-survival-analysis",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Extension to Survival Analysis",
    "text": "Extension to Survival Analysis\nTime-to-event outcomes are common in biomedical research and are often subject to right censoring due to loss to follow-up. In some situations, loss to follow-up occurs completely at random, known as noninformative censoring, which represents the simpler case. More often, the loss to follow-up depends on pre-baseline and post-baseline characteristics, referred to as informative censoring. For example, patients who are sicker at baseline or who develop treatment-related adverse events may be more likely to drop out early. In such situations, the censoring mechanism must be explicitly modeled as a function of baseline and time-dependent covariates. In this tutorial, we introduce a DR estimator for the simpler scenario with noninformative censoring under an observational study setting. We will discuss an alternative DR estimator for the informative censoring case in a future tutorial.\n\nPseudo-observation-based DR estimator\nWe introduce additional notations to discuss right-censored outcomes. Let \\(T\\) denote the survival time, \\(C\\) the censoring time, and \\(S_d(t)\\) the survival function under treatment \\(d\\) at time \\(t\\). We define the ATE as the difference in survival probabilities between treatment groups at time \\(t\\), i.e., \\(S_1(t)-S_0(t)\\).\nAssuming loss to follow-up is completely at random, i.e., \\(C \\perp T^{(d)}\\), Wang (2018) proposed a DR estimator using pseudo observations. The first step is to construct pseudo-observations for the survival function at time \\(t\\) using the Kaplan–Meier (KM) estimator: \\[\\hat{S}_d^{i}(t) = n \\hat{S}_d(t) - (n - 1) \\hat{S}^{-i}_{d}(t),\\] where \\(\\hat{S}_d(t)\\) is the KM estimate using all observations and \\(\\hat{S}^{-i}_d(t)\\) is the leave-one-out KM estimate with subject \\(i\\) removed. The pseudo-values \\(\\hat{S}_d^{i}(t)\\) are (asymptotically) individual-level contributions such that \\(E[\\hat{S}_d^{i}(t)|Z_i] \\approx \\hat{S}_d(t|Z_i)\\).\nThen, these pseudo-values replace the observed outcome \\(Y_i\\) and act as censoring-adjusted outcomes under the assumption of random censoring. With \\(\\hat{S}_d^{i}(t)\\) treated as the outcome, a standard DR estimator is then applied using a PS model and an outcome regression model. The pseudo-observation-based DR estimator for ATE is: \\[\n\\hat{\\tau}^{pseudo}_{DR} = \\frac{1}{n} \\sum_{i=1}^n\n\\left[\n  \\frac{D_i \\hat{S}_d^i(t)}{\\hat{e}(\\boldsymbol{Z}_i)}\n  - \\frac{(D_i - \\hat{e}(\\boldsymbol{Z}_i))}{\\hat{e}(\\boldsymbol{Z}_i)} \\hat{m}_1(t, \\boldsymbol{Z}_i)\n\\right]\n-\n\\left[\n  \\frac{(1 - D_i) \\hat{S}_d^i(t)}{1 - \\hat{e}(\\boldsymbol{Z}_i)}\n  + \\frac{(D_i - \\hat{e}(\\boldsymbol{Z}_i))}{1 - \\hat{e}(\\boldsymbol{Z}_i)} \\hat{m}_0(t, \\boldsymbol{Z}_i)\n\\right],\n\\] where \\(\\hat{m}_d(t, \\boldsymbol{Z}_i)\\) can be estimated using a Cox proportional hazards model and \\(\\hat{e}(\\boldsymbol{Z}_i))\\) can be estimated using logistic regression. The variance of the resulting estimator can be obtained based on Equation (17) in Wang (2018)."
  },
  {
    "objectID": "DRpseudo.html#implementation-of-the-dr-estimator",
    "href": "DRpseudo.html#implementation-of-the-dr-estimator",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Implementation of the DR estimator",
    "text": "Implementation of the DR estimator\nWe now demonstrate how to implement the pseudo-observation-based DR estimator under the random censoring scenario.\n\nData Generation\nWe first simulate the data according to the variable relationships depicted in the directed acyclic graph (DAG) below\n\n\nCode\ndag &lt;- dagitty(\"dag {\n  Z1 -&gt; Treatment\n  Z2 -&gt; Treatment\n  Z3 -&gt; Treatment\n  Z1 -&gt; Death\n  Z2 -&gt; Death\n  Z3 -&gt; Death\n  Z4 -&gt; Death\n  Treatment -&gt; Death\n}\")\n\ncoordinates(dag) &lt;- list(x = c(Z1 = 0, Z2 = 0.5, Z3 = 0, Z4 = 1.5, Treatment = 0.5, Death = 1.5),\n                         y = c(Z1 = 1, Z2 = 0.7, Z3 = 0, Z4 = 0, Treatment = 0.35, Death = 0.35))\n\nplot(dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nalpha_0 &lt;- -0.2\nalpha_Z1 &lt;- 0.6\nalpha_Z2 &lt;- 1.6\nalpha_Z3 &lt;- 1.6\nalpha_Z1Z2 &lt;- 0.8\nalpha_Z2sq &lt;- 0.5\n\nbeta_0 &lt;- -2.6      \nbeta_D &lt;- -0.8 \nbeta_Z1 &lt;- 0.2 \nbeta_Z2 &lt;- 1.9\nbeta_Z3 &lt;- 1.9 \nbeta_Z4 &lt;- 0.2\nbeta_Z2sq &lt;- 0.8\nbeta_Z1Z3 &lt;- 0.8\n\ngenerate_data_noninf &lt;- function(n = 500) {\n  Z1 &lt;- rnorm(n, 0, 1)\n  Z2 &lt;- rnorm(n, 0, 1)\n  Z3 &lt;- rnorm(n, 0, 1)\n  Z4 &lt;- rnorm(n, 0, 2) \n  Z5 &lt;- rnorm(n, 0, 2.5)\n  Z6 &lt;- rnorm(n, 0, 1)\n  Z7 &lt;- rnorm(n, 0, 1)\n  Z8 &lt;- rnorm(n, 0, 1)\n  Z9 &lt;- rnorm(n, 0, 1)\n  \n  logit_p &lt;- (alpha_0 + alpha_Z1 * Z1 + alpha_Z2 * Z2 + alpha_Z3 * Z3 + alpha_Z1Z2 * (Z1 * Z2) + alpha_Z2sq * (Z2^2)) / 2.8\n  p &lt;- plogis(logit_p) # exp(logit_p) / (1 + exp(logit_p))\n  D &lt;- rbinom(n, 1, p) # Treatment indicator\n  \n  hazard &lt;- exp(beta_0 + beta_D * D + beta_Z1 * Z1 + beta_Z2 * Z2 + beta_Z3 * Z3 + beta_Z4 * Z4 + beta_Z2sq * (Z2^2) + beta_Z1Z3 * (Z1 * Z3))\n  T_event &lt;- rexp(n, rate = hazard) # Event time from an exponential model\n  #summary(T_event)\n  \n  T_censor &lt;- rexp(n, rate = exp(-3)) # Censoring time based on random censoring\n  #summary(T_censor)\n  T_admin &lt;- rep(5, n) # Administrative (random) censoring\n  \n  Time &lt;- pmin(T_event, T_censor, T_admin) # Observed follow-up time\n  Event &lt;- as.numeric(T_event &lt;= T_censor & T_event &lt;= T_admin) # Event indicator\n  \n  return(data.frame(Z1 = Z1, Z2 = Z2, Z3 = Z3, Z4 = Z4, Z5 = Z5, Z6 = Z6, Z7 = Z7, Z8 = Z8, Z9 = Z9, D = D, Time = Time, Event = Event))\n}\n\n\n\n\nCode\ndata &lt;- generate_data_noninf()\n\nattr(data$Z1, \"label\") &lt;- \"Baseline value of Z1\"\nattr(data$Z2, \"label\") &lt;- \"Baseline value of Z2\"\nattr(data$Z3, \"label\") &lt;- \"Baseline value of Z3\"\nattr(data$Z4, \"label\") &lt;- \"Baseline value of Z4\"\nattr(data$Z5, \"label\") &lt;- \"Baseline value of Z5\"\nattr(data$Z6, \"label\") &lt;- \"Baseline value of Z6\"\nattr(data$Z7, \"label\") &lt;- \"Baseline value of Z7\"\nattr(data$Z8, \"label\") &lt;- \"Baseline value of Z8\"\nattr(data$Z9, \"label\") &lt;- \"Baseline value of Z9\"\nattr(data$D, \"label\") &lt;- \"Treatment indicator (1 = treated, 0 = control)\"\nattr(data$Time, \"label\") &lt;- \"Time index for longitudinal records\"\nattr(data$Event, \"label\") &lt;- \"Event indicator (1 = event occurred, 0 = otherwise)\"\n\nget_label &lt;- function(x) {\n  lbl &lt;- attr(x, \"label\", exact = TRUE)\n  if (is.null(lbl)) \"\" else as.character(lbl)\n}\n\ndict &lt;- data.frame(\n  Variable = names(data),\n  Meaning  = vapply(data, get_label, character(1)),\n  check.names = FALSE\n)\n\nknitr::kable(dict, caption = \"Data Dictionary\", row.names = FALSE) \n\n\n\nData Dictionary\n\n\nVariable\nMeaning\n\n\n\n\nZ1\nBaseline value of Z1\n\n\nZ2\nBaseline value of Z2\n\n\nZ3\nBaseline value of Z3\n\n\nZ4\nBaseline value of Z4\n\n\nZ5\nBaseline value of Z5\n\n\nZ6\nBaseline value of Z6\n\n\nZ7\nBaseline value of Z7\n\n\nZ8\nBaseline value of Z8\n\n\nZ9\nBaseline value of Z9\n\n\nD\nTreatment indicator (1 = treated, 0 = control)\n\n\nTime\nTime index for longitudinal records\n\n\nEvent\nEvent indicator (1 = event occurred, 0 = otherwise)\n\n\n\n\n\n\n\nCode\nsummary(data$Time)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; 0.000053 0.668455 3.640693 2.968875 5.000000 5.000000\n\ntable(data$Event)\n#&gt; \n#&gt;   0   1 \n#&gt; 286 214\n\n\n\n\nCode\ncensored_subjects &lt;- data[data$Event == 0, ]\n(early_dropout &lt;- sum(censored_subjects$Time &lt; 5))\n#&gt; [1] 74\n(admin_censored &lt;- sum(censored_subjects$Time &gt;= 5))\n#&gt; [1] 212\n\n\nAmong the 286 subjects with no events, 74 are early dropouts and 212 are administratively censored. Administrative censoring is random, so no additional modeling is needed. Early dropout could be covariate-dependent or occur completely at random, and we assume it is non-informative in this data generation.\n\n\nCode\nnrow(data[data$Event == 1 & data$Time &lt;= 3, ])\n#&gt; [1] 190\n\n\nWith the simulation above, we can calculate the true survival probability.\n\n\nCode\nget_true_survival &lt;- function(data, t, D) {\n  hazard &lt;- exp(beta_0 + beta_D * D + beta_Z1 * data$Z1 + beta_Z2 * data$Z2 + beta_Z3 * data$Z3 + beta_Z4 * data$Z4 + \n            beta_Z2sq * (data$Z2^2) + beta_Z1Z3 * (data$Z1 * data$Z3))\n  survival_probs &lt;- exp(-hazard * t) # baseline hazard is constant under an expotential survival model\n  return(mean(survival_probs)) \n}\n\n\nWe generate multiple datasets to compute the true ATEs at prespecified time points and corresponding 95% confidence intervals (CIs) using percentiles.\n\n\nCode\nn_datasets &lt;- 100\ndatasets &lt;- replicate(n_datasets, generate_data_noninf(), simplify = FALSE)\n\ntimes &lt;- c(1, 2, 3)\ntrue_ate_samples &lt;- matrix(NA, nrow = n_datasets, ncol = length(times))\n\nfor (i in 1:n_datasets) {\n  # Compute potential survival probabilities by setting treatment to 1 and 0 for all individuals\n  true_surv_1 &lt;- sapply(times, function(t) get_true_survival(datasets[[i]], t, D = 1))\n  true_surv_0 &lt;- sapply(times, function(t) get_true_survival(datasets[[i]], t, D = 0))\n  \n  # Calculate ATE as difference in survival probabilities\n  true_ate_samples[i, ] &lt;- true_surv_1 - true_surv_0\n}\n\ntrue_ate_mean &lt;- colMeans(true_ate_samples)\ntrue_ate_ci_lower &lt;- apply(true_ate_samples, 2, quantile, probs = 0.025)\ntrue_ate_ci_upper &lt;- apply(true_ate_samples, 2, quantile, probs = 0.975)\n\nresults_true &lt;- list(ATE = true_ate_mean, ci_lower = true_ate_ci_lower, ci_upper = true_ate_ci_upper)\n\ntrue_ate_with_ci &lt;- sapply(1:length(times), function(i) {\n  sprintf(\"%.3f (%.3f, %.3f)\", results_true$ATE[i], results_true$ci_lower[i], results_true$ci_upper[i])\n})\n\n\n\n\nUsing R package\nFirst, we implement the pseudo-observation-based DR estimator using the adjustedsurv function in the adjustedCurves R package.\n\n\nCode\npseudo_dr_ate_pkg &lt;- function(data, times, ps_correct = TRUE, or_correct = TRUE, use_bootstrap = TRUE) {\n  \n  data$D &lt;- factor(data$D, levels = c(0, 1))\n  \n  # Propensity score model\n  if (ps_correct) {\n    ps_model &lt;- glm(D ~ Z1 + Z2 + Z3 + I(Z2^2) + I(Z1*Z2), data = data, family = \"binomial\")\n  } else {\n    ps_model &lt;- glm(D ~ I(Z5 * Z6) + I(Z7 * Z8) + Z9 + I(Z9^2), data = data, family = \"binomial\")\n  }\n\n  # Covariates used in the outcome model\n  if (or_correct) {\n    data$Z1_Z3 &lt;- data$Z1 * data$Z3\n    data$Z2sq &lt;- data$Z2 * data$Z2\n    or_covars &lt;- c(\"Z1\", \"Z2\", \"Z3\", \"Z4\", \"Z1_Z3\", \"Z2sq\")\n  } else {\n    or_covars &lt;- c(\"Z5\")\n  }\n\n  # Pseudo-observation-based DR estimator for survival probabilities \n  # This method uses a generalized estimation equation for outcome model\n  adj_surv &lt;- adjustedsurv(\n    data = data,\n    variable = \"D\",\n    ev_time = \"Time\",\n    event = \"Event\",\n    method = \"aiptw_pseudo\",\n    treatment_model = ps_model,\n    outcome_vars = or_covars,\n    times = times, \n    conf_int = TRUE,\n    bootstrap = use_bootstrap,\n    n_boot = 100, # In practice, set n_boot to a larger value (e.g., 1000).\n    n_cores = if(use_bootstrap) n_cores else 1) \n  \n  # Treatment effect is defined as the difference in survival probabilities.\n  ate_results &lt;- adjusted_curve_diff(\n    adj = adj_surv,\n    group_1 = \"1\",  # Treated group\n    group_2 = \"0\",  # Control group\n    conf_int = TRUE,\n    conf_level = 0.95)\n  \n  ate_at_times &lt;- ate_results[ate_results$time %in% times, ]\n  \n  return(list(ATE = ate_at_times$diff, ci_lower = ate_at_times$ci_lower, ci_upper = ate_at_times$ci_upper))\n}\n\n\nWe estimate the ATEs at prespecified time points and corresponding 95% CIs.\n\n\nCode\nresults_package &lt;- pseudo_dr_ate_pkg(data, times)  \n\nate_with_ci &lt;- sapply(1:length(times), function(i) {\n  sprintf(\"%.3f (%.3f, %.3f)\", results_package$ATE[i], results_package$ci_lower[i], results_package$ci_upper[i])\n})\n\nresults_manual_tbl &lt;- data.frame(\n  ATE = c(\"Estimated\", \"True\"),\n  `T = 1` = c(ate_with_ci[1], true_ate_with_ci[1]), \n  `T = 2` = c(ate_with_ci[2], true_ate_with_ci[2]),\n  `T = 3` = c(ate_with_ci[3], true_ate_with_ci[3]), \n  check.names = FALSE)\n\nkable(results_manual_tbl, caption = \"ATE (95% CI)\", align = c('c', 'c', 'c', 'c')) %&gt;% \n  kable_styling(full_width = FALSE, font_size = 12)\n\n\n\nATE (95% CI)\n\n\nATE\nT = 1\nT = 2\nT = 3\n\n\n\n\nEstimated\n0.127 (0.060, 0.194)\n0.047 (-0.026, 0.120)\n0.069 (-0.006, 0.145)\n\n\nTrue\n0.074 (0.068, 0.082)\n0.087 (0.079, 0.094)\n0.094 (0.085, 0.102)\n\n\n\n\n\n\n\nManual coding\nThe pseudo-observation-based DR estimator can be implemented manually\n\nWe use a Cox regression for the outcome model rather than GEE as in the package above\n\n\n\nCode\npseudo_dr_ate_manual &lt;- function(data, times, ps_correct = TRUE, or_correct = TRUE) {\n  \n  # Outcome regression (Cox proportional hazard model)\n  if (or_correct) {\n    or_model &lt;- coxph(Surv(Time, Event) ~ D + Z1 + Z2 + Z3 + Z4 + I(Z2^2) + I(Z1 * Z3), data = data)\n  } else {\n    or_model &lt;- coxph(Surv(Time, Event) ~ Z5, data = data)\n  }\n  \n  data1 &lt;- data\n  data1$D &lt;- 1\n  \n  data0 &lt;- data\n  data0$D &lt;- 0\n  \n  # Extract the linear predictors (Beta^T * Z) from fitted Cox models \n  lp_1 &lt;- predict(or_model, newdata = data1, type = \"lp\")\n  lp_0 &lt;- predict(or_model, newdata = data0, type = \"lp\")\n  \n  # Baseline survival functions\n  bh &lt;- basehaz(or_model, centered = FALSE)\n  bh$surv &lt;- exp(-bh$hazard)\n  \n  # Baseline survival probabilities at different times\n  s0_f &lt;- stepfun(bh$time, c(1, bh$surv))\n  s0_t &lt;- s0_f(times)\n  \n  # Predicted survival probabilities\n  m1 &lt;- sapply(s0_t, function(s0) s0 ^ exp(lp_1))\n  m0 &lt;- sapply(s0_t, function(s0) s0 ^ exp(lp_0))\n  \n  # Propensity scores\n  if (ps_correct) {\n    ps_model &lt;- glm(D ~ Z1 + Z2 + Z3 + I(Z2^2) + I(Z1*Z2), data = data, family = \"binomial\")\n  } else {\n    ps_model &lt;- glm(D ~ I(Z5 * Z6) + I(Z7 * Z8) + Z9 + I(Z9^2), data = data, family = \"binomial\")\n  }\n  e &lt;- predict(ps_model, type = \"response\")\n  \n  # Pseudo-observations (individual estimates of the survival function at specified time points)\n  pseudo_obs &lt;- pseudosurv(time = data$Time, event = data$Event, tmax = times)$pseudo\n  \n  surv_1 &lt;- numeric(length(times))\n  surv_0 &lt;- numeric(length(times))\n  ATE &lt;- numeric(length(times))\n  \n  for(j in 1:length(times)) {\n    # DR for treated group\n    term1_treated &lt;- data$D * pseudo_obs[, j] / e\n    term2_treated &lt;- (data$D - e) * m1[, j] / e\n    term_full_treated &lt;- term1_treated - term2_treated\n    surv_1[j] &lt;- mean(term_full_treated)\n    \n    # DR for control group  \n    term1_control &lt;- (1 - data$D) * pseudo_obs[, j] / (1 - e)\n    term2_control &lt;- (data$D - e) * m0[, j] / (1 - e)\n    term_full_control &lt;- term1_control + term2_control\n    surv_0[j] &lt;- mean(term_full_control) \n    \n    ATE[j] &lt;- surv_1[j] - surv_0[j]\n  }\n  return(list(ATE = ATE))\n}\n\n\nWe calculate the standard error (SE) of the ATE using bootstrapping\n\n\nCode\nse_bootstrap &lt;- function(data, times, num_boot = 100) {\n  # In practice, set n_boot to a larger value (e.g., 1000).\n  n &lt;- nrow(data)\n  boot_results &lt;- matrix(NA, nrow = num_boot, ncol = length(times))\n\n  for (b in 1:num_boot) {\n    boot_indices &lt;- sample(1:n, size = n, replace = TRUE)\n    boot_data &lt;- data[boot_indices, ]\n    boot_results[b, ] &lt;- pseudo_dr_ate_manual(data = boot_data, times = times)$ATE\n  }\n\n  se &lt;- apply(boot_results, 2, sd, na.rm = TRUE)\n  return(list(se = se))\n}\n\n\nWe calculate the 95% CI of the ATE using parametric bootstrapping assuming ATE is normally distributed\n\n\nCode\nresults_manual &lt;- pseudo_dr_ate_manual(data, times)  \nse_manual &lt;- se_bootstrap(data, times)\n\n# Calculate 95% confidence intervals\nresults_manual$ci_lower &lt;- results_manual$ATE - 1.96 * se_manual$se\nresults_manual$ci_upper &lt;- results_manual$ATE + 1.96 * se_manual$se\n\nate_with_ci &lt;- sapply(1:length(times), function(i) {\n  sprintf(\"%.3f (%.3f, %.3f)\", results_manual$ATE[i], \n  results_manual$ci_lower[i], results_manual$ci_upper[i])})\n\nresults_manual_tbl &lt;- data.frame(\n  ATE = c(\"Estimated\", \"True\"),\n  `T = 1` = c(ate_with_ci[1], true_ate_with_ci[1]), \n  `T = 2` = c(ate_with_ci[2], true_ate_with_ci[2]),\n  `T = 3` = c(ate_with_ci[3], true_ate_with_ci[3]), \n  check.names = FALSE)\n\nkable(results_manual_tbl, caption = \"ATE (95% CI)\", align = c('c', 'c', 'c', 'c')) %&gt;% \n  kable_styling(full_width = FALSE, font_size = 12)\n\n\n\nATE (95% CI)\n\n\nATE\nT = 1\nT = 2\nT = 3\n\n\n\n\nEstimated\n0.116 (0.053, 0.178)\n0.050 (-0.016, 0.116)\n0.079 (0.016, 0.142)\n\n\nTrue\n0.074 (0.068, 0.082)\n0.087 (0.079, 0.094)\n0.094 (0.085, 0.102)\n\n\n\n\n\n\n\nCrude estimation\nFit a crude Cox model with no covariate or propensity score adjustment to show the amount of confounding bias in our data\n\n\nCode\ncrude_cox_estimator &lt;- function(data, times) {\n\n  crude_cox &lt;- coxph(Surv(Time, Event) ~ D, data = data)\n  treatment_effect &lt;- coef(crude_cox)[\"D\"]\n  \n  base_surv &lt;- survfit(crude_cox)\n  s0 &lt;- summary(base_surv, times = times, extend = TRUE)$surv\n  \n  # Survival for treated group: S_baseline^exp(beta_D)\n  surv_1 &lt;- s0^exp(treatment_effect)\n  \n  # Survival for control group: S_baseline^exp(0) \n  surv_0 &lt;- s0\n  \n  ATE &lt;- surv_1 - surv_0\n\n  return(list(ATE = ATE))\n}\n\nresults_crude &lt;- crude_cox_estimator(data, times)\n\n\n\n\nEvaluate the performance of pseudo-observation-based DR estimator\nWe use the datasets generated previously to calculate RMSE, bias, and variance.\n\n\nCode\nplan(multisession, workers = n_cores)\n\ncalculate_metrics &lt;- function(datasets, true_ate_samples, times, method = \"package\", ps_correct = TRUE, or_correct = TRUE) {\n  \n  n_datasets &lt;- length(datasets)\n  \n  ate &lt;- future_lapply(1:n_datasets, function(i) {\n    library(survival)\n    library(pseudo)\n    library(adjustedCurves)\n    \n    est_results &lt;- switch(\n      method,\n      \"package\" = pseudo_dr_ate_pkg(datasets[[i]], times, ps_correct = ps_correct, or_correct = or_correct,\n                                    use_bootstrap = FALSE),\n      \"manual\" = pseudo_dr_ate_manual(datasets[[i]], times, ps_correct = ps_correct, or_correct = or_correct),\n      \"crude\" = crude_cox_estimator(datasets[[i]], times),\n      stop(\"Unknown: \", method))\n    \n    est_results$ATE\n  }, future.seed = TRUE)\n  \n  ate_all &lt;- do.call(rbind, ate)\n  \n  bias &lt;- colMeans(ate_all - true_ate_samples)\n  variance &lt;- apply(ate_all, 2, var)\n  rmse &lt;- sqrt(colMeans((ate_all - true_ate_samples)^2))\n  \n  return(list(bias = bias, variance = variance, rmse = rmse))\n}\n\n\n\n\nModel misspecification scenarios\nTo illustrate the robustness of the DR estimator to model misspecification, we compare 4 scenarios:\n\nScenario 1: Both the outcome model and the propensity score model are correctly specified.\nScenario 2: The propensity score model is correctly specified, but the outcome model is misspecified.\nScenario 3: The outcome model is correctly specified, while the propensity score model is misspecified.\nScenario 4: Both models are misspecified.\n\n\n\nCode\nmetrics_1_package &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", \n                                       ps_correct = TRUE, or_correct = TRUE)\nmetrics_2_package &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", \n                                       ps_correct = TRUE, or_correct = FALSE)\nmetrics_3_package &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", \n                                       ps_correct = FALSE, or_correct = TRUE)\nmetrics_4_package &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"package\", \n                                       ps_correct = FALSE, or_correct = FALSE)\n\nmetrics_1_manual &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"manual\", \n                                      ps_correct = TRUE, or_correct = TRUE)\nmetrics_2_manual &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"manual\", \n                                      ps_correct = TRUE, or_correct = FALSE)\nmetrics_3_manual &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"manual\", \n                                      ps_correct = FALSE, or_correct = TRUE)\nmetrics_4_manual &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"manual\", \n                                      ps_correct = FALSE, or_correct = FALSE)\n\nmetrics_crude &lt;- calculate_metrics(datasets, true_ate_samples, times, method = \"crude\")\n\n\n\n\nCode\nscenarios &lt;- c(\n  \"0: Crude Cox (Unadjusted)\",\n  \"1: Both Correct\",\n  \"2: PS Correct, OR Incorrect\",\n  \"3: PS Incorrect, OR Correct\",\n  \"4: Both Incorrect\")\n\n# Combine values across all time points into a single string\nformat_values &lt;- function(vec) {\n  paste(sprintf(\"%.4f\", vec), collapse = \", \")\n}\n\n# RMSE table\nrmse_package &lt;- c(\"—\", format_values(metrics_1_package$rmse), format_values(metrics_2_package$rmse), \n                  format_values(metrics_3_package$rmse), format_values(metrics_4_package$rmse))\n\nrmse_manual &lt;- c(format_values(metrics_crude$rmse), format_values(metrics_1_manual$rmse), \n                 format_values(metrics_2_manual$rmse), format_values(metrics_3_manual$rmse), \n                 format_values(metrics_4_manual$rmse))\n\nrmse_table &lt;- data.frame(Scenario = scenarios, Package = rmse_package, Manual = rmse_manual)\n\nkable(rmse_table,\n      col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n      caption = \"Performance of the estimator (RMSE)\",\n      align = c('c', 'c', 'c')) %&gt;%\n  kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n  add_header_above(c(\" \" = 1, \"Package\" = 1, \"Manual\" = 1))\n\n\n\nPerformance of the estimator (RMSE)\n\n\n\n\n\n\n\n\n\nPackage\n\n\nManual\n\n\n\nScenario\nT = 1, 2, 3\nT = 1, 2, 3\n\n\n\n\n0: Crude Cox (Unadjusted)\n—\n0.1883, 0.2243, 0.2450\n\n\n1: Both Correct\n0.0247, 0.0270, 0.0266\n0.0257, 0.0277, 0.0287\n\n\n2: PS Correct, OR Incorrect\n0.0376, 0.0346, 0.0346\n0.0364, 0.0337, 0.0337\n\n\n3: PS Incorrect, OR Correct\n0.0253, 0.0268, 0.0272\n0.0367, 0.0330, 0.0324\n\n\n4: Both Incorrect\n0.2178, 0.2348, 0.2431\n0.2176, 0.2346, 0.2429\n\n\n\n\n\n\n\nCode\n# Bias table\nbias_package &lt;- c(\"—\", format_values(metrics_1_package$bias), format_values(metrics_2_package$bias), \n                  format_values(metrics_3_package$bias), format_values(metrics_4_package$bias))\n\nbias_manual &lt;- c(format_values(metrics_crude$bias), format_values(metrics_1_manual$bias), \n                 format_values(metrics_2_manual$bias), format_values(metrics_3_manual$bias), \n                 format_values(metrics_4_manual$bias))\n\nbias_table &lt;- data.frame(Scenario = scenarios, Package = bias_package, Manual = bias_manual)\n\nkable(bias_table,\n      col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n      caption = \"Performance of the estimator (Bias)\",\n      align = c('c', 'c', 'c')) %&gt;%\n  kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n  add_header_above(c(\" \" = 1, \"Package\" = 1, \"Manual\" = 1))\n\n\n\nPerformance of the estimator (Bias)\n\n\n\n\n\n\n\n\n\nPackage\n\n\nManual\n\n\n\nScenario\nT = 1, 2, 3\nT = 1, 2, 3\n\n\n\n\n0: Crude Cox (Unadjusted)\n—\n-0.1854, -0.2210, -0.2415\n\n\n1: Both Correct\n0.0011, 0.0035, 0.0027\n0.0014, 0.0038, 0.0029\n\n\n2: PS Correct, OR Incorrect\n-0.0001, 0.0026, 0.0020\n0.0001, 0.0027, 0.0021\n\n\n3: PS Incorrect, OR Correct\n0.0010, 0.0034, 0.0026\n-0.0247, -0.0176, -0.0136\n\n\n4: Both Incorrect\n-0.2141, -0.2309, -0.2391\n-0.2138, -0.2306, -0.2389\n\n\n\n\n\n\n\nCode\n# Variance table\nvar_package &lt;- c(\"—\", format_values(metrics_1_package$variance), format_values(metrics_2_package$variance), \n                 format_values(metrics_3_package$variance), format_values(metrics_4_package$variance))\n\nvar_manual &lt;- c(format_values(metrics_crude$variance), format_values(metrics_1_manual$variance), \n                format_values(metrics_2_manual$variance), format_values(metrics_3_manual$variance), \n                format_values(metrics_4_manual$variance))\n\nvar_table &lt;- data.frame(Scenario = scenarios, Package = var_package, Manual = var_manual)\n\nkable(var_table,\n      col.names = c(\"Scenario\", \"T = 1, 2, 3\", \"T = 1, 2, 3\"),\n      caption = \"Performance of the estimator (Variance)\",\n      align = c('c', 'c', 'c')) %&gt;%\n  kable_styling(full_width = FALSE, font_size = 12) %&gt;%\n  add_header_above(c(\" \" = 1, \"Package\" = 1, \"Manual\" = 1))\n\n\n\nPerformance of the estimator (Variance)\n\n\n\n\n\n\n\n\n\nPackage\n\n\nManual\n\n\n\nScenario\nT = 1, 2, 3\nT = 1, 2, 3\n\n\n\n\n0: Crude Cox (Unadjusted)\n—\n0.0011, 0.0014, 0.0018\n\n\n1: Both Correct\n0.0006, 0.0008, 0.0007\n0.0007, 0.0008, 0.0008\n\n\n2: PS Correct, OR Incorrect\n0.0014, 0.0012, 0.0012\n0.0014, 0.0012, 0.0011\n\n\n3: PS Incorrect, OR Correct\n0.0007, 0.0008, 0.0008\n0.0007, 0.0008, 0.0009\n\n\n4: Both Incorrect\n0.0016, 0.0019, 0.0020\n0.0016, 0.0019, 0.0020"
  },
  {
    "objectID": "DRpseudo.html#funding",
    "href": "DRpseudo.html#funding",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Funding",
    "text": "Funding\nThis work was supported by Utah Clinical & Translational Science Institute (CTSI) Translational Innovation Pilot (TIP) Program Award (NCATS UM1TR004409)."
  },
  {
    "objectID": "DRpseudo.html#footnotes",
    "href": "DRpseudo.html#footnotes",
    "title": "Doubly Robust Estimation of Causal Effects in Survival Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎\nCorresponding author: yizhe.xu@hsc.utah.edu↩︎"
  }
]